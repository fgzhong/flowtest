spark.sql.adaptive.enabled=false
spark.sql.adaptive.shuffle.targetPostShuffleInputSize=67108864b
spark.sql.autoBroadcastJoinThreshold=10485760
spark.sql.broadcastTimeout=300
spark.sql.cbo.enabled=false
spark.sql.cbo.joinReorder.dp.star.filter=false
spark.sql.cbo.joinReorder.dp.threshold=12
spark.sql.cbo.joinReorder.enabled=false
spark.sql.cbo.starSchemaDetection=false
spark.sql.columnNameOfCorruptRecord=_corrupt_record
spark.sql.crossJoin.enabled=false
spark.sql.extensions=
spark.sql.files.ignoreCorruptFiles=false
spark.sql.files.maxPartitionBytes=134217728
spark.sql.files.maxRecordsPerFile=0
spark.sql.groupByAliases=true
spark.sql.groupByOrdinal=true
spark.sql.hive.caseSensitiveInferenceMode=INFER_AND_SAVE
spark.sql.hive.filesourcePartitionFileCacheSize=262144000
spark.sql.hive.manageFilesourcePartitions=true
spark.sql.hive.metastorePartitionPruning=true
spark.sql.hive.thriftServer.singleSession=false
spark.sql.hive.verifyPartitionPath=false
spark.sql.optimizer.metadataOnly=true
spark.sql.orc.filterPushdown=false
spark.sql.orderByOrdinal=true
spark.sql.parquet.binaryAsString=false
spark.sql.parquet.cacheMetadata=true
spark.sql.parquet.compression.codec=snappy
spark.sql.parquet.enableVectorizedReader=true
spark.sql.parquet.filterPushdown=true
spark.sql.parquet.int64AsTimestampMillis=false
spark.sql.parquet.int96AsTimestamp=true
spark.sql.parquet.mergeSchema=false
spark.sql.parquet.respectSummaryFiles=false
spark.sql.parquet.writeLegacyFormat=false
spark.sql.pivotMaxValues=10000
spark.sql.session.timeZone=Asia/Shanghai
spark.sql.shuffle.partitions=200
spark.sql.sources.bucketing.enabled=true
spark.sql.sources.default=parquet
spark.sql.sources.parallelPartitionDiscovery.threshold=32
spark.sql.sources.partitionColumnTypeInference.enabled=true
spark.sql.statistics.fallBackToHdfs=false
spark.sql.streaming.checkpointLocation=
spark.sql.streaming.metricsEnabled=false
spark.sql.streaming.numRecentProgressUpdates=100
spark.sql.thriftserver.scheduler.pool=
spark.sql.thriftserver.ui.retainedSessions=200
spark.sql.thriftserver.ui.retainedStatements=200
spark.sql.variable.substitute=true
spark.sql.warehouse.dir=...
spark.sql.inMemoryColumnarStorage.compressed=true 为每一列选择一种压缩编码方式
spark.sql.inMemoryColumnarStorage.batchSize=10000 缓存批处理大小， 较大的批处理可以提高内存利用率和压缩率，但同时也会带来 OOM(Out Of Memory)的风险
spark.sql.files.maxPartitionBytes=128M  读取文件时单个分区可容纳的最大字节数
spark.sql.files.openCostinBytes=4M  打开文件的估算成本，按照同一时间能够扫描的字节数来测量，当往一个分区写入多个文件时会使用，高估相对较好，这样小文件分区将会比大文件分区速度更快（优先调度）
spark.sql.autoBroadcastJoinThreshold=10M 用于配置一个表在执行 join 操作时能够广播给所有 worker 节点的最大字节大小，通地将这个值设置为-1可以禁用广播
spark.sql.shuffle.partitions=200 用于配置 join 或聚合操作混洗（shuffle）数据时使用的分区数