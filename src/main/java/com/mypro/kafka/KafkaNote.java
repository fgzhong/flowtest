package com.mypro.kafka;

/**
 * @author fgzhong
 * @description: kafka
 * @since 2019/7/16
 */
public class KafkaNote {

    /*
     是否允许消息丢失、重复、延迟和吞吐量
     1、流式特点
       1、发布和订阅流式的记录
       2、存储流式的记录，并有较好的容错性
       3、可以在流式记录产生时就进行处理
     2、kafka应用场景
       1、构造实时流数据管道，它可以在系统或应用之间可靠地获取数据
       2、构建实时流式应用程序，对这些流式数据进行转换或者影响
     3、kafka特点
       1、作为一个集群，运行在一台或者多台服务器上
       2、kafka通过topic对存储的流数据进行分类
       3、每条记录中包含一个key，一个value和一个timestamp
     4、核心API
       1、ProducerMain API 允许一个应用程序发布一串流式的数据到一个或者多个Kafka topic
       2、 Consumer API 允许一个应用程序订阅一个或多个 topic ，并且对发布给他们的流式数据进行处理
       3、 Streams API 允许一个应用程序作为一个流处理器，消费一个或者多个topic产生的输入流，然后生产一个输出流到一个或多个topic中去，在输入输出流中进行有效的转换
       4、 Connector API 允许构建并运行可重用的生产者或者消费者，将Kafka topics连接到已存在的应用程序或者数据系统。比如，连接到一个关系型数据库，捕捉表（table）的所有变更内容
     5、Topics和日志
       1、topic —— 串流式记录
         1、topic就是数据主题，是数据记录发布的地方，可以区分业务系统。Kafka中的Topics总是多订阅者模式，一个topic可以拥有一个或者多个消费者来订阅它的数据
         2、对于每一个topic， Kafka集群都会维持一个分区日志
         3、每个分区都是有序且顺序不可变的记录集，并且不断地追加到结构化的commit log文件。分区中的每一个记录都会分配一个id号来表示顺序，我们称之为offset，offset用来唯一的标识分区中每一条记录
         4、Kafka 集群保留所有发布的记录—无论他们是否已被消费—并通过一个可配置的参数——保留期限来控制. 举个例子， 如果保留策略设置为2天，一条记录发布后两天内，可以随时被消费，两天过后这条记录会被抛弃并释放磁盘空间。Kafka的性能和数据大小无关，所以长时间存储数据没有什么问题
         5、在每一个消费者中唯一保存的元数据是offset（偏移量）即消费在log中的位置.偏移量由消费者所控制:通常在读取记录后，消费者会以线性的方式增加偏移量，但是实际上，由于这个位置由消费者控制，所以消费者可以采用任何顺序来消费记录。例如，一个消费者可以重置到一个旧的偏移量，从而重新处理过去的数据；也可以跳过最近的记录，从"现在"开始消费
         6、消费者的增加和减少，对集群或者其他消费者没有多大的影响
         7、日志中的 partition（分区）用途
           1、当日志大小超过了单台服务器的限制，允许日志进行扩展。
             每个单独的分区都必须受限于主机的文件限制，
             不过一个主题可能有多个分区，因此可以处理无限量的数据
           2、可以作为并行的单元集
      6、分布式
        1、日志的分区partition （分布）在Kafka集群的服务器上。
           每个服务器在处理数据和请求时，共享这些分区。
           每一个分区都会在已配置的服务器上进行备份，确保容错性
        2、每个分区都有一台 server 作为 “leader”，零台或者多台server作为 follwers 。
           leader server 处理一切对 partition （分区）的读写请求，
           而follwers只需被动的同步leader上的数据
           当leader宕机了，followers 中的一台服务器会自动成为新的 leader
           每台 server 都会成为某些分区的 leader 和某些分区的 follower，因此集群的负载是平衡的
      7、ProducerMain
        1、生产者可以将数据发布到所选择的topic（主题）中
        2、生产者负责将记录分配到topic的哪一个 partition（分区 —— 不指定则根据key）中
        3、可以使用循环的方式来简单地实现负载均衡，也可以根据某些语义分区函数(例如：记录中的key)来完成
        4、参数
          1、acks=0/1/all 指定多少个Partition副本收到消息才认为写入成功。
                acks=0，生产者不需要等待服务器的响应，以网络能支持的最大速度发送消息，吞吐量高，但是如果broker没有收到消息，生产者是不知道的
                acks=1，leader partition收到消息，生产者就会收到一个来自服务器的成功响应
                acks=all，所有的partition都收到消息，生产者才会收到一个服务器的成功响应
          2、buffer.memory 设置生产者内缓存区域的大小，生产者用它缓冲要发送到服务器的消息
          3、compression.type，默认情况下，消息发送时不会被压缩，该参数可以设置成snappy、gzip或lz4对发送给broker的消息进行压缩
          4、retries，生产者从服务器收到临时性错误时，生产者重发消息的次数
          5、batch.size，发送到同一个partition的消息会被先存储在batch中，该参数指定一个batch可以使用的内存大小，单位是byte。不一定需要等到batch被填满才能发送
          6、linger.ms，生产者在发送消息前等待linger.ms，从而等待更多的消息加入到batch中。如果batch被填满或者linger.ms达到上限，就把batch中的消息发送出去
          7、max.in.flight.requests.per.connection，生产者在收到服务器响应之前可以发送的消息个数
      8、Consumer
        1、消费者使用一个 消费组 名称来进行标识，发布到topic中的每条记录被分配给订阅消费组中的一个消费者实例.消费者实例可以分布在多个进程中或者多个机器上
        2、如果所有的消费者实例在同一消费组中，消息记录会负载平衡到每一个消费者实例
        3、如果所有的消费者实例在不同的消费组中，每条消息记录会广播到所有的消费者进程.
      kafka只允许单个partition的数据被一个consumer线程消费
    */
}
